<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Flash Attention | Yifei Zuo</title>
    <link rel="stylesheet" href="/css/style.css" />
    <link rel="stylesheet" href="/css/fonts.css" />
  </head>
  <head>
    <meta charset="utf-8">
    <style>
        .custom-link {
            color: black; /* Default text color */
            text-decoration: underline; /* Removes underline */
        }
        .custom-link:hover {
            color: white; /* Text color on hover (could be different if desired) */
            background-color: black; /* Background color on hover */
        }
    </style>
  </head>
  <body>

<main>
<span style="font-weight:bold;font-size:28px">Language Model Training At Scale</span><span style="float: right; font-weight: bold"><span style="float: right; font-weight: bold"><a href="/" class="custom-link" style="text-decoration: none;">About</a> <a href="/posts" class="custom-link">Posts</a></span></span></br>
<!-- <span class="title">Training Language Model At Scale</span> -->
<!-- <span style="float: right; font-weight: bold"><a href="/" style="color: black; font-weight: bold; text-decoration: none;">About</a> <a href="/posts" style="color: black;font-weight: bold; text-decoration: none;">Posts</a></span></br></br> -->
<h2 id="Flash Attention">Flash Attention</h2>
Suppose that we are computing the attention outputs with length <code>$n$</code> and head dimension <code>$d$</code>.
Let <code>$b$</code> denotes the block size of the tile, <code>$t$</code> denotes the number of tiles in a row.
<code>$$
\begin{align*}
s_i &\leftarrow Q[k,:]K^T[:,i]\\
m_i &\leftarrow \max\{m_{i-1}, x_i\}\\
d_i &\leftarrow d_{i-1} e^{m_{i-1} - m_i} + e^{x_i - m_i}\\
o_i &\leftarrow o_{i-1} \frac{d_{i-1}e^{m_{i-1}-m_i}}{d_i} + \frac{e^{x_i-m_i}}{d_i}V[i,:]
\end{align*}
$$</code>
Notably, the overall SRAM memory footprint depends only on <code>$b$</code> and <code>$d$</code> and is not related to context length <code>$n$</code>. As a
result, this algorithm can scale to long context without encountering memory issues.
<div style="text-align: center">
<img src="flash_attn.png" width="300px"></img>
</div>
<pre>
<code class="language-python">def rotate_half(x):
    x1 = x[..., : x.shape[-1] // 2]
    x2 = x[..., x.shape[-1] // 2 :]
    return torch.cat((-x2, x1), dim=-1)

def apply_rotary_pos_emb(q, k, cos, sin, position_ids):
    cos = cos.squeeze(1).squeeze(0)
    sin = sin.squeeze(1).squeeze(0)
    cos = cos[position_ids].unsqueeze(1)
    sin = sin[position_ids].unsqueeze(1)
    q_embed = (q * cos) + (rotate_half(q) * sin)
    k_embed = (k * cos) + (rotate_half(k) * sin)
    return q_embed, k_embed
</code>
</pre>
</main>

<footer>
<script defer src="//yihui.org/js/math-code.js"></script>
<script defer src="//mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

<script defer src="//yihui.org/js/center-img.js"></script>
  <hr/>
  Â© <a href="https://dune-z.github.io">Yifei Zuo</a> 2022 &ndash; 2024
  </footer>
  </body>
</html>