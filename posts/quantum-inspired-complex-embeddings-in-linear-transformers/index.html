<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Quantum-Inspired Linear Attention with Complex Embeddings | Yifei Ethan Zuo</title>
    <link rel="stylesheet" href="/css/style.css" />
    <link rel="stylesheet" href="/css/fonts.css" />
    
  </head>

  <body>
    <nav>
    <ul class="menu">
      
      <li><a href="/">Home</a></li>
      
      <li><a href="/posts">Post</a></li>
      
    </ul>
    <hr/>
    </nav>

<div class="article-meta">
<h1><span class="title">Quantum-Inspired Linear Attention with Complex Embeddings</span></h1>

<h2 class="date">2023/09/18</h2>
</div>

<main>
<p>Recent advancements in long-context language models emphasize the importance of positional encoding, allowing the Transformer architecture to grasp long-range dependencies. The seminal paper &ldquo;<em>Attention is All You Need</em>&rdquo; introduced positional encoding as an external piece of information since the original Transformer architecture is permutation invariant. Furthermore, to encode distance relatively, the Rotary Positional Embedding (RoPE) was proposed. We will see that RoPE fits more intuitively within the framework of linear recurrence as a result of internal assumptions instead of external inductive-bias. And we will derive complex linear Attention based on this framework.</p>
<blockquote>
<p>Some notational conventions:</p>
<ul>
<li>Capital letters are used for matrices. They&rsquo;re real-valued unless otherwise specified.</li>
<li>We use lowercase letters for vectors and use Dirac notation if they are complex-valued. The inner product of Hilbert space is denoted as <code>$\langle \cdot \rvert \cdot \rangle$</code>.</li>
<li>For Transformers, we use <code>$ q , k, v$</code> to denote the query, key and value projections of input <code>$ x$</code> respectively. The hidden dimension is <code>$d$</code> by default.</li>
</ul>
</blockquote>
<p>State space method is a powerful tool for analyzing dynamic systems. Albert Gu introduced SSM (State Space Model) into time series and language modeling. A typical SSM is defined as follows:</p>
<p><code>$$ \begin{aligned} \frac{d s}{dt} &amp;= A s + B u \\ y &amp;= C s + D u \end{aligned} $$</code></p>
<p>Where <code>$s$</code> is the state representation with system input <code>$u$</code> and output <code>$y$</code>. Linear attention is a state space model with a specific input and output format. Here we only consider linear attention with linear kernel.</p>
<p><code>$$ \begin{aligned} \frac{d S}{dt} &amp;= A s + k v^T \\ \mathsf{linearAttn}(x_t) &amp;= q_t^T S \end{aligned} $$</code></p>
<p>The solution to the above linear differential equation with zero initial condition is a convolution.</p>
<p><code>$$ \begin{aligned} s &amp;= \int_0^t e^{A(t-\tau)} k v^T d\tau \\ \mathsf{linearAttn}(x) &amp;= \int_0^t q^T_te^{A(t-\tau)} k v^T d\tau \\ &amp;= \int_0^t q^T_tUe^{\Lambda(t-\tau)}U^H k v^T d\tau \\ \end{aligned} $$</code></p>
<p>Calculating the power series <code>$e^{A(t-\tau)}$</code> is computationally intensive. A natural remedy involves diagonalization. Assume that <code>$A$</code> can be diagonalized in complex field. Here we decompose <code>$A$</code> as <code>$ A= U \Lambda U^H$</code>, where <code>$U$</code> is unitary and <code>$ \Lambda$</code> is diagonal matrix of eigenvalues. The diagonalizability, which requires the the sum of the geometric multiplicities of all its eigenvalues equals to <code>$d$</code>, is a mild assumption considering the fact that the set of all matrices with distinct eigenvalues is dense in the set of all matrices. Specifically, we have the following lemma.</p>
<p><strong>Lemma (Theorem 1 in Hartfiel (1995)).</strong> <em>Real matrices with <code>$k$</code> distinct nonzero eigenvalues are dense in the set of all <code>$d\times d$</code> real matrices with rank at most <code>$k$</code>, where <code>$0&lt;k\le d$</code>.</em></p>
<p>If we further assume that all eigenvalues are exclusively complex (absence of real eigenvalues), and owing to the properties of eigenvalue decomposition, these complex eigenvalues, paired with their respective eigenvectors, come in conjugate pairs. The following orthogonal basis transformation converts the matrix <code>$U, \Lambda$</code> into its real-valued counterpart. For each 2-d subspace, the transformation is as follows:</p>
<p><code>$$ \begin{aligned} \begin{bmatrix} \mathbf v_i &amp; \mathbf v_i^* \end{bmatrix}\begin{bmatrix} \lambda_i &amp; 0 \\ 0 &amp; \lambda_i^* \end{bmatrix}=\begin{bmatrix} \mathfrak R\{\mathbf v_i\} &amp; \mathfrak I\{\mathbf v_i\} \end{bmatrix}\begin{bmatrix} \alpha_i\begin{pmatrix} cos\phi_i &amp; -sin\phi_i\\ sin\phi_i &amp; cos\phi_i \end{pmatrix} \end{bmatrix} \end{aligned} $$</code></p>
<p>This transform facilitates the merging of the unitary matrix with the query and key projection matrix <code>$W_q, W_k$</code>. This process offers a comparable interpretation to RoPE, especially when the eigenvectors of <code>$A$</code> are normalized to be unitary i.e. <code>$\alpha_i=1, i=1,2,\dots,\frac{d}{2}$</code>. If scaled by a consistently decaying factor <code>$\gamma$</code>, it parallels the retentive mechanism. We denote the rotation matrix <code>$e^{\Lambda}=\mathcal{R}_{C}$</code> and use <code>$\mathcal R$</code> to denote the transformed rotation matrix by convention. The eventual linear attention mechanism converted in real field is as follows.</p>
<p><code>$$ \begin{aligned} \mathsf{linearAttn}(x_t) &amp;= \int_0^t q^T_t\mathcal R^{t-\tau} k v^T d\tau \xrightarrow[\text{}]{\text{discrete}} \sum_{m=1}^n q^T_n\mathcal R^{n-m} k_mv_m^T \end{aligned} $$</code></p>
<p>For clarity, we just use the complex formulation and merge the <code>$U, U^H$</code> matrices into the query and key projection <code>$W_q, W_k$</code>, resulting in query and key embeddings in complex form. For consistency, we also convert the value embedding into complex field as well.</p>
<p><code>$$ \begin{aligned} \mathsf{linearAttn}(x_n) &amp;= \sum_{m=1}^n \langle q_n\rvert \mathcal R_C^{n-m} \lvert k_m\rangle \lvert v_m\rangle,\text{ where }\lvert q_n\rangle, \lvert k_m\rangle, \lvert v_m\rangle \in \mathbb C^{\frac{d}{2}} \end{aligned} $$</code></p>
<p>In the standard attention methodology, we employ the softmax function on the real part of query-key inner product which convert the real value series into a probability distribution. Specifically, softmax functions result in a Boltzmann distribution which maximizes the system&rsquo;s entropy, under the constraint that the expected microstate energy remains constant. To be specific, the element in attention weight is as follows:</p>
<p><code>$$ \alpha_j = \frac{\exp(\mathfrak{R}(\langle q_n\rvert \mathcal R_C^{n-j} \lvert k_j\rangle))}{\sum_{m=1}^n \exp(\mathfrak{R}(\langle q_n\rvert \mathcal R_C^{n-m} \lvert k_m\rangle))} $$</code></p>
<p>It is somewhat counterintuitive that the standard Attention mechanism exclusively employs the real part of the complex inner product<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>. What implications arise if we also incorporate the imaginary part? Can it offer any added advantages? Note that there isn&rsquo;t a definitive linear operation that converts a series of real values directly into a probability distribution and therefore non-linearity is inevitable.</p>
<p>We hypothesize that the exponential distribution class introduces a specific inductive bias, which may not be optimal for all tasks, especially in long context scenarios where standard attention mechanisms are prone to forgetting in the middle of the sequence. Fortunately, by retaining the imaginary part of the complex inner product, we can leverage a well-defined probabilistic distribution, which is grounded in the deep physical insights provided by quantum mechanics.</p>
<p><code>$$ \begin{aligned} \mathsf{linearAttn}(x_n) &amp;=\sum_{m=1}^n \lvert v_m\rangle \langle q_n\rvert \mathcal R_C^{m-n} \lvert k_m\rangle \\ &amp;= \sum_{m=1}^n C_{n,m} e^{\phi j}\lvert v_m\rangle \\ &amp;= \sum_{m=1}^n \frac{C_{n,m}}{\sqrt{\sum_{m=1}^n \lvert C_{n,m}}\rvert^2} [\underbrace{(\sqrt{\sum_{m=1}^n \lvert C_{n,m}\rvert^2}) e^{\phi j}}_{\text{can be merged into }W_v}\lvert v_m\rangle] \end{aligned} $$</code></p>
<p>Given that <code>$\sum_{m=1}^n(\frac{C_{n,m}}{\sqrt{\sum_{m=1}^n \lvert C_{n,m}\rvert^2}})^2=1$</code> and <code>$C_{n,m}\in \mathbb R$</code> is non-negative, the coefficient transforms into a probability distribution for a random variable in Hilbert space. In practical implementations, instead of directly determining the coefficient&rsquo;s normalization factor, one could opt for applying group normalization to each head. This approach holds the potential for the model to self-learn the normalization process for the coefficient.<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup> From here, we can decouple the complex embedding into its real counterpart by placing the real and imaginary components in separate dimensions. This adjustment allows for seamless integration with the multi-head attention mechanism, where probabilities are determined by projecting the state vector onto different subspaces and computing the squared length of this projection.</p>
<p>The backward pass is operated in complex field as a result, which requires us to properly calculate the gradient of complex matrix. Empirically, we can just take the real part and imaginary part as separate matrices and calculate their gradients respectively. But that does not represent the true gradient if the complex function is not holomorphic.</p>
<p>Specifically, consider function <code>$f: \mathbb C \to \mathbb C$</code> s.t. <code>$f(z)=u(x,y)+jv(x,y)$</code>, where <code>$z=x+jy$</code> and <code>$u,v$</code> are real-valued functions.</p>
<p><code>$$ \begin{aligned} \nabla f(z) \neq  \end{aligned} $$</code></p>
<p><code>$$ \begin{aligned} \newcommand{\pd}[2]{\frac{\partial #1}{\partial #2}} \newcommand{\pl}[1]{\pd{\mathcal L}{#1}} \newcommand{\vecr}[1]{\mathsf{vec}(#1)} \vecr{\pl{\bar V}} &amp;= \vecr{\pl{O}\color{blue}{\pd{O}{\bar V}} + \pl{\bar O}\pd{\bar O}{\bar V}} \\ &amp;= (\mathbb I \otimes P^H) \vecr{\pl{\bar O}} \\ &amp;= \vecr{P^H\pl{\bar O}}\\ \vecr{\pl{\bar P}} &amp;= \vecr{\pl{O}\color{blue}{\pd{O}{\bar P}} + \pl{\bar O}\pd{\bar O}{\bar P}} \\ &amp;= (\bar V\otimes \mathbb I) \vecr{\pl{\bar O}} \\ &amp;= \vecr{\pl{\bar O} V^H} \end{aligned} $$</code></p>
<p>Therefore we get
<code>$$ \begin{aligned} \newcommand{\pd}[2]{\frac{\partial #1}{\partial #2}} \newcommand{\pl}[1]{\pd{\mathcal L}{#1}} \newcommand{\vecr}[1]{\mathsf{vec}(#1)} \pl{\bar V} &amp;= P^H\pl{\bar O} \\ \pl{\bar P} &amp;= \pl{\bar O} V^H \\ \end{aligned} $$</code></p>
<p>Similarly, we calculate the gradient of matrix <code>$Q, K$</code>:</p>
<p><code>$$ \begin{aligned} \newcommand{\pd}[2]{\frac{\partial #1}{\partial #2}} \newcommand{\pl}[1]{\pd{\mathcal L}{#1}} \newcommand{\vecr}[1]{\mathsf{vec}(#1)} \vecr{\pl{\bar Q}} &amp;= \vecr{\pl{P}\color{blue}{\pd{P}{\bar Q}} + \pl{\bar P}\pd{\bar P}{\bar Q}} \\ &amp;= (K^T \otimes \mathbb I) \vecr{\pl{\bar P}} \\ &amp;= \vecr{\pl{\bar P} K} \\ \vecr{\pl{\bar K}} &amp;= \vecr{\pl{P}\pd{P}{\bar K} + \pl{\bar P}\color{blue}{\pd{\bar P}{\bar K}}} \\ &amp;= (\mathbb I\otimes Q) \vecr{\pl{P}} \\ &amp;= \vecr{Q \pl{P}} \end{aligned} $$</code></p>
<p>Therefore we get
<code>$$ \begin{aligned} \newcommand{\pd}[2]{\frac{\partial #1}{\partial #2}} \newcommand{\pl}[1]{\pd{\mathcal L}{#1}} \newcommand{\vecr}[1]{\mathsf{vec}(#1)} \pl{\bar Q} &amp;= \pl{\bar P} K \\ \pl{\bar K} &amp;= Q \pl{P} \\ \end{aligned} $$</code></p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>It should be clarified that while retentive networks express their formulas in the complex domain, their actual implementation only involves real part. Refer to their official implementation <a href="https://github.com/microsoft/torchscale">https://github.com/microsoft/torchscale</a> for more details.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>We just need to make the training process stable .e.g. avoid gradient explosion/vanishing and numerical instability.&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>

</main>

  <footer>
  <script defer src="//yihui.org/js/math-code.js"></script>
<script defer src="//mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

<script defer src="//yihui.org/js/center-img.js"></script>
  
  <hr/>
  © <a href="https://dune-z.github.io">Yifei Ethan Zuo</a> 2022 &ndash; 2023
  
  </footer>
  </body>
</html>

