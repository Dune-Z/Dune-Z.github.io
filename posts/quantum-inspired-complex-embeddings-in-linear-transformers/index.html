<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Quantum-Inspired Linear Attention with Complex Embeddings | Yifei Ethan Zuo</title>
    <link rel="stylesheet" href="/css/style.css" />
    <link rel="stylesheet" href="/css/fonts.css" />
    
  </head>

  <body>
    <nav>
    <ul class="menu">
      
      <li><a href="/">Home</a></li>
      
      <li><a href="/posts">Post</a></li>
      
    </ul>
    <hr/>
    </nav>

<div class="article-meta">
<h1><span class="title">Quantum-Inspired Linear Attention with Complex Embeddings</span></h1>


</div>

<main>
<p>In the realm of long-context language models, the significance of positional encoding has grown paramount. It provides the Transformer architecture with the ability to discern long-range dependencies. The foundational paper, &ldquo;<em>Attention is All You Need</em>,&rdquo; posited positional encoding as an ancillary input, a necessity owing to the permutation invariance of the original Transformer design. Enhancing this further, the Rotary Positional Embedding (RoPE) emerged to provide a relative distance encoding. Techniques such as positional interpolation have underscored RoPE&rsquo;s pivotal role in modulating context length.</p>
<p>A conspicuous challenge with the standard Transformer in long-context applications lies in its quadratic complexity associated with attention matrix computations with respect to token length. The memory overhead, dominated by the KV cache, impedes scalability to more extended contexts. Linear attention presents a viable solution, touting a consistent memory footprint and linear complexity per iteration.</p>
<p>We will see that RoPE can be seamlessly integrates within the linear recurrence framework, attributed more to intrinsic assumptions rather than externally imposed biases. This article will extrapolate on this observation, culminating in a derivation of a complex linear attention grounded in this framework. <strong>Our empirical results demonstrate that this approach outperforms the standard Transformer and Retentive Networks with scale 1.3B and larger.</strong></p>
<blockquote>
<p>Some notational conventions:</p>
<ul>
<li>Capital letters are used for matrices. They&rsquo;re real-valued unless otherwise specified.</li>
<li>We use lowercase letters for vectors and use Dirac notation if they are complex-valued. The inner product of Hilbert space is denoted as <code>$\langle \cdot \rvert \cdot \rangle$</code>.</li>
<li>For Transformers, we use <code>$ q , k, v$</code> to denote the query, key and value projections of input <code>$ x$</code> respectively. The hidden dimension is <code>$d$</code> by default.</li>
</ul>
</blockquote>
<h2 id="linear-attention-as-a-recurrent-model">Linear Attention as a Recurrent Model</h2>
<p>State space method is a powerful tool for analyzing dynamic systems. Linear state space model (SSM) can effectively model any time-invariant linear system. A typical SSM is defined as follows:</p>
<p><code>$$ \begin{aligned} \frac{d s}{dt} &amp;= A s + B u \\ y &amp;= C s + D u \end{aligned} $$</code></p>
<p>Where <code>$s$</code> is the state representation with system input <code>$u$</code> and output <code>$y$</code>.
Albert Gu introduced SSM into time series and language modeling. To be specific, they consider transition matrix <code>$A$</code> a <em>high-order polynomial projection operators (HiPPO)</em> which achieves optimal memorization capacity with respect to some pre-define measurements and demonstrate powerful performance in long-context language modeling.</p>
<p>Linear attention is a recurrent model like SSM but with a specific input format and a little twist on the output relation. The recurrent state is updated with rank-1 matrix in each decoding step and the output is multiplicative with data-dependent matrix.
Here we only consider linear attention with linear kernel.</p>
<p><code>$$ \begin{aligned} \frac{d S}{dt} &amp;= A s + k v^T \\ \mathsf{linearAttn}(x_t) &amp;= q_t^T S \end{aligned} $$</code></p>
<p>The solution to the above linear differential equation with zero initial condition is a convolution.</p>
<p><code>$$ \begin{aligned} S &amp;= \int_0^t e^{A(t-\tau)} k v^T d\tau \\ \mathsf{linearAttn}(x) &amp;= \int_0^t q^T_te^{A(t-\tau)} k v^T d\tau \\ &amp;= \int_0^t q^T_tUe^{\Lambda(t-\tau)}U^H k v^T d\tau \\ \xrightarrow[\text{}]{\text{discrete}} &amp;= \sum_{m=1}^n q^T_nU\Lambda^{n-m}U^H k_m v_m^T \end{aligned} $$</code></p>
<p>Calculating the power series <code>$e^{A(t-\tau)}$</code> is computationally intensive. A natural remedy involves diagonalization. Assume that <code>$A$</code> can be diagonalized in complex field. Here we decompose <code>$A$</code> as <code>$ A= U \Lambda U^H$</code>, where <code>$U$</code> is unitary and <code>$ \Lambda$</code> is diagonal matrix of eigenvalues. The diagonalizability, which requires the the sum of the geometric multiplicities of all its eigenvalues equals to <code>$d$</code>, is a mild assumption considering the fact that the set of all matrices with distinct eigenvalues is dense in the set of all matrices. Specifically, we have the following lemma.</p>
<p><strong>Lemma (Theorem 1 in Hartfiel (1995)).</strong> <em>Real matrices with <code>$k$</code> distinct nonzero eigenvalues are dense in the set of all <code>$d\times d$</code> real matrices with rank at most <code>$k$</code>, where <code>$0&lt;k\le d$</code>.</em></p>
<p>If we further assume that all eigenvalues are exclusively complex (absence of real eigenvalues), and owing to the properties of eigenvalue decomposition, these complex eigenvalues, paired with their respective eigenvectors, come in conjugate pairs. The following orthogonal basis transformation converts the matrix <code>$U, \Lambda$</code> into its real-valued counterpart. For each 2-d subspace, the transformation is as follows:</p>
<p><code>$$ \begin{aligned} \begin{bmatrix} \mathbf v_i &amp; \mathbf v_i^* \end{bmatrix}\begin{bmatrix} \lambda_i &amp; 0 \\ 0 &amp; \lambda_i^* \end{bmatrix}=\begin{bmatrix} \mathfrak R\{\mathbf v_i\} &amp; \mathfrak I\{\mathbf v_i\} \end{bmatrix}\begin{bmatrix} \alpha_i\begin{pmatrix} cos\phi_i &amp; -sin\phi_i\\ sin\phi_i &amp; cos\phi_i \end{pmatrix} \end{bmatrix} \end{aligned} $$</code></p>
<p>This transform facilitates the merging of the unitary matrix with the query and key projection matrix <code>$W_q, W_k$</code>. This process offers a comparable interpretation to RoPE, especially when the eigenvectors of <code>$A$</code> are normalized to be unitary i.e. <code>$\alpha_i=1, i=1,2,\dots,\frac{d}{2}$</code>. If scaled by a consistently decaying factor <code>$\gamma$</code>, it parallels the retentive mechanism. We denote the rotation matrix <code>$\mathcal{R}_{C}$</code> as a rotation operator in Hilbert space and we use <code>$\mathcal R$</code> to denote the transformed rotation matrix by convention, which is used as an element-wise operator. The eventual converted linear attention mechanism is as follows.</p>
<p><code>$$ \begin{aligned} \mathsf{linearAttn}(x_t) &amp;= \int_0^t q^T_t\mathcal R^{t-\tau} k v^T d\tau \xrightarrow[\text{}]{\text{discrete}} \sum_{m=1}^n q^T_n\mathcal R^{n-m} k_mv_m^T \end{aligned} $$</code></p>
<p>For clarity, we just use the complex formulation and merge the <code>$U, U^H$</code> matrices into the query and key projection <code>$W_q, W_k$</code>, resulting in query and key embeddings in complex form. For consistency, we also convert the value embedding into complex field as well.</p>
<p><code>$$ \begin{aligned} \mathsf{linearAttn}(x_n) &amp;= \sum_{m=1}^n \langle q_n\rvert \mathcal R_C^{n-m} \lvert k_m\rangle \lvert v_m\rangle\\ \text{ where }&amp;\lvert q_n\rangle, \lvert k_m\rangle, \lvert v_m\rangle \in \mathbb C^{\frac{d}{2}} \end{aligned} $$</code></p>
<h2 id="softmax-attention">Softmax Attention</h2>
<p>In the standard attention methodology, we employ the softmax function on the real part of query-key inner product which convert the real value series into a probability distribution. Specifically, softmax functions result in a <em>Boltzmann distribution</em> which maximizes the system&rsquo;s entropy, under the constraint that the expected microstate energy <code>$\mathbb E[\epsilon]$</code> remains constant.</p>
<p><code>$$ p_i = \frac{1}{Z} e^{-\frac{\epsilon_i}{kT}} $$</code></p>
<p>Softmax in Transformer transform a series a value into Boltzmann distribution with fix <em>temperature</em>. To be specific, the element in attention weight is as follows:</p>
<p><code>$$ \alpha_j = \frac{\exp(\mathfrak{R}((\langle q_n\rvert \mathcal R_C^{n-j} \lvert k_j\rangle-M)/\sqrt{d}))}{\sum_{m=1}^n \exp(\mathfrak{R}((\langle q_n\rvert \mathcal R_C^{n-m} \lvert k_m\rangle-M))/\sqrt{d})} $$</code></p>
<p>where <code>$M$</code> is a large value that keeps the exponential part negative so that we can compute the softmax function stably.</p>
<p>Note that there isn&rsquo;t a definitive linear operation that converts a series of real values directly into a probability distribution and therefore non-linearity is inevitable. The exponential distribution class introduces a specific inductive bias, which may not be optimal for all tasks, especially in long context scenarios where standard attention mechanisms are prone to forgetting in the middle of the sequence.</p>
<p><strong>Proposition 1.</strong> <em>Sample complexity of softmax.</em></p>
<p><strong>Proposition 2.</strong> <em>Calibration .</em></p>
<h2 id="quantum-inspired-complex-embeddings">Quantum-Inspired Complex Embeddings</h2>
<p>It is somewhat counterintuitive that the standard Attention mechanism exclusively employs the real part of the complex inner product<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>. What implications arise if we also incorporate the imaginary part? Can it offer any added advantages?</p>
<p>Fortunately, by retaining the imaginary part of the complex inner product, we can leverage a well-defined probabilistic distribution, which is grounded in the deep physical insights provided by quantum mechanics, where probabilities are determined by projecting the state vector onto different subspaces and computing the squared length of this projection.</p>
<p>In <code>$d$</code>-dimensional quantum system with Hilbert space <code>$\mathcal H$</code>. Consider <code>$\mathcal B=\{\lvert\phi_i\rangle\}_{i=1}^d$</code> be an orthogonal basis of <code>$\mathcal H$</code>. For example, <code>$\mathcal B$</code> could be given by the orthogonal eigenstates of a hermitian operator such as Hermiltonian <code>$H$</code>. Any state can be expressed as <em>coherent superposition</em> with complex coefficients <code>$c_i\in\mathbb C$</code>:</p>
<p><code>$$ \newcommand{\state}[1]{\lvert #1\rangle} \state{\psi} = c_1 \state{\phi_1} + c_2 \state{\phi_2} + \dots + c_d \state{\phi_d} $$</code></p>
<p>In quantum mechanics, we consider quantum state that are normalized to have unit length i.e. <code>$\langle\psi\rvert\psi\rangle=\sum_{j=1}^d \vert c_h\rvert^2=1$</code>. According to the Born interpretation of wave function, the square of the coefficient represents the probability of finding the system in the corresponding eigenstate upon measurement in the considered basis <code>$\mathcal B$</code>. Such unit vectors are <em>pure states</em> which contains all the available physical information about the system.</p>
<p>Basic quantum mechanics in defined in the framework of quantum probability with single additional rule. If a physical system is independent and autonomous, then quantum mechanics postulates that it has a pure state space <code>$\mathcal H$</code> and its state evolves according to a one-parameter group <code>$U(t)$</code> of unitary operations, which has the form of</p>
<p><code>$$ \begin{aligned} U(t) = e^{-itH} \end{aligned} $$</code></p>
<p>for some <em>Hamiltonian</em> <code>$H$</code>. And the system evolves according to the <em>Schrödinger equation</em>:</p>
<p><code>$$ \begin{aligned} i\hbar\frac{\partial}{\partial t}\lvert \psi\rangle = H\lvert \psi\rangle \end{aligned} $$</code></p>
<p>We can build our attention system without referring to the quantum mechanics itself since the quantum universe has their own physical rule and we can just use the framework of quantum probability or <code>$C^*$</code>-<em>Algebra</em>. But it should be interesting to explore the connection between the two in the future.</p>
<p><code>$$ \begin{aligned} \mathsf{linearAttn}(x_n) &amp;=\sum_{m=1}^n \lvert v_m\rangle \langle q_n\rvert \mathcal R_C^{m-n} \lvert k_m\rangle \\ &amp;= \sum_{m=1}^n C_{n,m} e^{\phi j}\lvert v_m\rangle \\ &amp;= \sum_{m=1}^n \frac{C_{n,m}}{\sqrt{\sum_{m=1}^n \lvert C_{n,m}}\rvert^2} [{(\sqrt{\sum_{m=1}^n \lvert C_{n,m}\rvert^2}) e^{\phi j}}\lvert v_m\rangle] \end{aligned} $$</code></p>
<p>Given that <code>$\sum_{m=1}^n(\frac{C_{n,m}}{\sqrt{\sum_{m=1}^n \lvert C_{n,m}\rvert^2}})^2=1$</code> and <code>$C_{n,m}\in \mathbb R$</code> is non-negative, the coefficient transforms into a probability distribution for a random variable in <code>$\mathcal H$</code>.</p>
<p>In practical implementations, instead of directly determining the coefficient&rsquo;s normalization factor or normalizing the state, one could opt for applying group normalization to each head. This approach holds the potential for the model to self-learn the normalization process for the coefficient, therefore we just use the unnormalized states.
From here, we can decouple the complex embedding into its real counterpart by placing the real and imaginary components in separate dimensions. This adjustment allows for seamless integration with the multi-head attention mechanism.</p>
<h2 id="possible-extensions">Possible Extensions</h2>
<h3 id="i-position-interpolation">I. Position Interpolation</h3>
<p>We consider the position interpolation in our model where the limit of time step <code>$\Delta t \to 0$</code>. We can just consider the continue formulation of the linear attention mechanism where the matrix power should be replaced by matrix exponential <code>$e^{tA}$</code>.</p>
<p>We consider the transition matrix <code>$A$</code> a skew-symmetric matrix as a special case. The eigenvalue of a skew-symmetric matrix is purely imaginary or zero and therefore the matrix exponential can be calculated as <code>$Ue^{t\Theta}U^H=Ue^{-it\Psi}U^H$</code> where <code>$\Psi$</code> is real diagonal and irrelative to <code>$t$</code>. The token embedding <code>$\lvert x\rangle$</code> in position <code>$t$</code> can be represented as follows:</p>
<p><code>$$ \lvert x(t)\rangle = e^{-it\Psi} \lvert x\rangle $$</code></p>
<h3 id="ii-fine-tuning">II. Fine-tuning</h3>
<p>Initial state is a critical factor affecting the trajectory evolution in phase space. Above derivation is based on the zero initial condition. None zero initial condition will result in a different trajectory and can be seen as we have pre-define tokens in the sequence. This approach parallels the <em>llama-adapter</em> which fine-tines the model with pivot tokens.
Since our model is a recurrent model with rank-1 update, we can just fine-tune the initial state in low-rank fashion where the rank of the initial state <code>$r$</code> is the number of pivot tokens.</p>
<p><code>$$ \begin{aligned} S_t &amp;= S_0 e^{tA} + \int_0^t e^{(t-\tau)A} k v^T d\tau \\ &amp;= W_aW_b^T e^{tA} + \int_0^t e^{(t-\tau)A} k v^T d\tau \\ \end{aligned} $$</code></p>
<p>where <code>$W_a, W_b\in \mathbb C^{\frac{d}{2}\times r}$</code>.</p>
<h3 id="iii-in-context-learning">III. In-Context Learning</h3>
<p>Transformer architecture has two dimension of information flow, one is layer-wise and the other is token-wise. NeuralODE models the layer to layer information flow with skip-connection with differential equation when the layer is deep enough. We demonstrate that the token to token information flow can be modeled with differential equation as well and in-context learning can be achieved by collecting perturbation in token information flow and calculate gradient descent in layer information flow.</p>
<h2 id="model-details">Model Details</h2>
<h3 id="backward-pass">Backward Pass</h3>
<p>The backward pass is operated in complex field as a result, which requires us to properly calculate the gradient of complex matrix. Empirically, we can just take the real part and imaginary part as separate matrices and calculate their gradients respectively. In our experiment this approach does optimize the network, but it is not theoretically correct in terms of gradient calculation of complex functions.</p>
<p>Moreover, consider function <code>$f: \mathbb C \mapsto \mathbb C$</code> s.t. <code>$f(z)=u(x,y)+jv(x,y)$</code>, where <code>$z=x+jy$</code> and <code>$u,v$</code> are real-valued functions. In order for the complex derivative of function <code>$f$</code> to exist in standard holomorphic sense, the real partial derivates of <code>$u$</code> and <code>$v$</code> must exist and satisfy the <em>Cauchy-Riemann condition</em>, which is a strong condition and is not satisfied by most functions. In order to extend the complex derivative to functions that are not holomorphic, we typically use <em>Wirtinger derivative</em> which can be reduced to the standard partial derivative when the function is holomorphic. We here consider functions over conjugate coordinates</p>
<p><code>$$ c := (z, \bar z)^T \in \mathbb C \times \mathbb C $$</code>
The Wirtinger derivative of function <code>$f$</code> can be formally defined as the following pair of partial derivatives:</p>
<p><code>$$ \begin{align} &amp;\frac{\partial f(z, \bar z)}{\partial z}\rvert_{\bar z=\text{const}} \\ &amp;\frac{\partial f(z, \bar z)}{\partial \bar z}\rvert_{z=\text{const}} \end{align} $$</code></p>
<p>This can be extended to multi-variable case naturally. The cogradient operator and its conjugate over high-dimensional complex coordinates are defined as follows:</p>
<p><code>$$ \begin{aligned} \frac{\partial}{\partial \mathbf z} &amp;:= (\frac{\partial}{\partial z_1} \dots \frac{\partial}{\partial z_n}) = \frac{\partial}{\partial \mathbf x} - j\frac{\partial}{\partial \mathbf y} \\ \frac{\partial}{\partial \overline{\mathbf z}} &amp;:= (\frac{\partial}{\partial \bar z_1} \dots \frac{\partial}{\partial \bar z_n}) = \frac{\partial}{\partial \mathbf x} + j\frac{\partial}{\partial \mathbf y} \end{aligned} $$</code></p>
<p>When we do optimization on real-valued loss, the step we should take while making variable update is given by the conjugate of the cogradient.</p>
<p><code>$$ \nabla f = f - \eta \frac{\partial f}{\partial \overline{\mathbf z}} $$</code></p>
<p>The detailed backward calculation is as follows:</p>
<p><code>$$ \begin{aligned} \newcommand{\pd}[2]{\frac{\partial #1}{\partial #2}} \newcommand{\pl}[1]{\pd{\mathcal L}{#1}} \newcommand{\vecr}[1]{\mathsf{vec}(#1)} \vecr{\pl{\bar V}} &amp;= \vecr{\pl{O}\color{blue}{\pd{O}{\bar V}} + \pl{\bar O}\pd{\bar O}{\bar V}} \\ &amp;= (\mathbb I \otimes P^H) \vecr{\pl{\bar O}} \\ &amp;= \vecr{P^H\pl{\bar O}}\\ \vecr{\pl{\bar P}} &amp;= \vecr{\pl{O}\color{blue}{\pd{O}{\bar P}} + \pl{\bar O}\pd{\bar O}{\bar P}} \\ &amp;= (\bar V\otimes \mathbb I) \vecr{\pl{\bar O}} \\ &amp;= \vecr{\pl{\bar O} V^H} \end{aligned} $$</code></p>
<p>Therefore we get
<code>$$ \begin{aligned} \newcommand{\pd}[2]{\frac{\partial #1}{\partial #2}} \newcommand{\pl}[1]{\pd{\mathcal L}{#1}} \newcommand{\vecr}[1]{\mathsf{vec}(#1)} \pl{\bar V} &amp;= P^H\pl{\bar O} \\ \pl{\bar P} &amp;= \pl{\bar O} V^H \\ \end{aligned} $$</code></p>
<p>Similarly, we calculate the gradient of matrix <code>$Q, K$</code>:</p>
<p><code>$$ \begin{aligned} \newcommand{\pd}[2]{\frac{\partial #1}{\partial #2}} \newcommand{\pl}[1]{\pd{\mathcal L}{#1}} \newcommand{\vecr}[1]{\mathsf{vec}(#1)} \vecr{\pl{\bar Q}} &amp;= \vecr{\pl{P}\color{blue}{\pd{P}{\bar Q}} + \pl{\bar P}\pd{\bar P}{\bar Q}} \\ &amp;= (K^T \otimes \mathbb I) \vecr{\pl{\bar P}} \\ &amp;= \vecr{\pl{\bar P} K} \\ \vecr{\pl{\bar K}} &amp;= \vecr{\pl{P}\pd{P}{\bar K} + \pl{\bar P}\color{blue}{\pd{\bar P}{\bar K}}} \\ &amp;= (\mathbb I\otimes Q) \vecr{\pl{P}} \\ &amp;= \vecr{Q \pl{P}} \end{aligned} $$</code></p>
<p>Therefore we get
<code>$$ \begin{aligned} \newcommand{\pd}[2]{\frac{\partial #1}{\partial #2}} \newcommand{\pl}[1]{\pd{\mathcal L}{#1}} \newcommand{\vecr}[1]{\mathsf{vec}(#1)} \pl{\bar Q} &amp;= \pl{\bar P} K \\ \pl{\bar K} &amp;= Q \pl{P} \\ \end{aligned} $$</code></p>
<p>TODO: wrong derivation of K.
TODO: reference to papers.</p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>It should be clarified that while retentive networks express their formulas in the complex domain, their actual implementation only involves real part. Refer to their official implementation <a href="https://github.com/microsoft/torchscale">https://github.com/microsoft/torchscale</a> for more details.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>

</main>

  <footer>
  <script defer src="//yihui.org/js/math-code.js"></script>
<script defer src="//mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

<script defer src="//yihui.org/js/center-img.js"></script>
  
  <hr/>
  © <a href="https://dune-z.github.io">Yifei Ethan Zuo</a> 2022 &ndash; 2023
  
  </footer>
  </body>
</html>

